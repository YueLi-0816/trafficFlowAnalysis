{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Make defination "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read detector(buffer)_POI  join data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_detector_POI_join_file(detector_POI_join_shp):\n",
    "    detector_POI_join = gpd.read_file(\"../20211028_SCOOT_Data/SCOOT_data_points-shp/Spatial Join/\" + detector_POI_join_shp + \".shp\")\n",
    "    \n",
    "    return detector_POI_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Get POI num & percentage of each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_on_POI(detector_POI_join):\n",
    "\n",
    "    valid_points = detector_POI_join.loc[detector_POI_join.JOIN_FID != -1, 'siteId']\n",
    "    points = sorted(list(set(valid_points)))\n",
    "\n",
    "    df_list = []\n",
    "    for i in points:\n",
    "        b = detector_POI_join.loc[detector_POI_join.siteId == i]\n",
    "        c = dict(Counter(b['groupname']))  #count each group of each site\n",
    "        percent_c = {key: value/len(b) for key, value in c.items()} #convert count to percentage    \n",
    "        percent_c['POI_num'] = len(b)\n",
    "        percent_c['siteId'] = i\n",
    "        each_df = pd.DataFrame(percent_c, index=[i])\n",
    "        df_list.append(each_df) \n",
    "\n",
    "    df_POI_percent = pd.concat(df_list, ignore_index=True, sort=False).fillna(0)\n",
    "    df_POI_percent = df_POI_percent[['siteId', 'POI_num', 'Public Infrastructure', 'Commercial Services', 'Education and Health', 'Transport', 'Retail', 'Sport and Entertainment', 'Accommodation, Eating and Drinking', 'Manufacturing and Production', 'Attractions']]\n",
    "    df_POI_percent = df_POI_percent.sort_values(by=['siteId']).reset_index(drop=True)\n",
    "\n",
    "    return df_POI_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Get the degree of mixing POI (entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_POI_mix(df_POI_percent):\n",
    "\n",
    "    df_POI_percent['POI_mix'] = 0\n",
    "    \n",
    "    for i in range(len(df_POI_percent)):\n",
    "        POI_percent = df_POI_percent.iloc[i,2:].tolist()\n",
    "        POI_percent = [x for x in POI_percent if x != 0]\n",
    "        POI_mix = entropy(POI_percent)/np.log(len(POI_percent))\n",
    "        df_POI_percent.loc[i, 'POI_mix'] = POI_mix\n",
    "    \n",
    "    all_df_POI_percent = df_POI_percent.fillna(0)\n",
    "    return all_df_POI_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Get statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sta(df_POI_percent, buffer_size):\n",
    "    POI_sta = df_POI_percent.describe().T[['mean','std','min','max']]\n",
    "    POI_sta = POI_sta.add_suffix('_' + buffer_size)\n",
    "    return POI_sta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Get POI percent of all buffer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = ['100', '200', '300', '400']\n",
    "df_list = []\n",
    "for doc in doc_list:\n",
    "    detector_POI_join = read_detector_POI_join_file('useful_detector_POI_join_' + doc)\n",
    "    df_POI_percent = percentage_on_POI(detector_POI_join)\n",
    "    all_df_POI_percent = get_POI_mix(df_POI_percent)\n",
    "    \n",
    "    site_list =  all_df_POI_percent.siteId.tolist()\n",
    "    all_df_POI_percent = all_df_POI_percent.drop('siteId', axis=1).add_suffix('_' + doc)\n",
    "    \n",
    "    df_list.append(all_df_POI_percent)\n",
    "\n",
    "all_POI = pd.concat(df_list, axis=1).fillna(0)\n",
    "all_POI.insert(loc=0, column='siteId', value=site_list)\n",
    "all_POI\n",
    "\n",
    "#all_POI.to_csv('all_buffer_POI_percent.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = ['100', '200', '300', '400']\n",
    "df_list = []\n",
    "for doc in doc_list:\n",
    "    detector_POI_join = read_detector_POI_join_file('useful_detector_POI_join_' + doc)\n",
    "    df_POI_percent = percentage_on_POI(detector_POI_join)\n",
    "    all_df_POI_percent = get_POI_mix(df_POI_percent)\n",
    "    POI_sta = get_sta(all_df_POI_percent, doc)\n",
    "    df_list.append(POI_sta)\n",
    "\n",
    "all_sta = pd.concat(df_list, axis=1)\n",
    "all_sta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landcover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Make defination "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read buffer_landcover intersect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_landcover_intersect_file(buffer_landcover_intersect_csv):\n",
    "    landcover_intersect = gpd.read_file(\"../20211028_SCOOT_Data/SCOOT_data_points-shp/Spatial Join/\" + buffer_landcover_intersect_csv + \".csv\")\n",
    "    \n",
    "    return landcover_intersect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Organise landcover percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landcover_percent(landcover_intersect):\n",
    "    \n",
    "    sites = list(set(landcover_intersect.siteId))\n",
    "\n",
    "    df_list = []\n",
    "    for site in sites:\n",
    "        each_site_landcover = landcover_intersect.loc[landcover_intersect['siteId'] == site].loc[:,['class_2018','PERCENTAGE']]\n",
    "        landcover_dict = dict(each_site_landcover.values)\n",
    "        landcover_dict['siteId'] = site\n",
    "        landcover_df = pd.DataFrame(landcover_dict, index=[site])\n",
    "        df_list.append(landcover_df) \n",
    "\n",
    "    df_landcover_percent = pd.concat(df_list, ignore_index=True, sort=False).fillna(0)\n",
    "\n",
    "    #change the column order\n",
    "    site_list = df_landcover_percent.siteId.tolist()\n",
    "    df_landcover_percent = df_landcover_percent.drop('siteId', axis=1).astype(float)/100 #convert to float\n",
    "    df_landcover_percent.insert(loc=0, column='siteId', value=site_list)\n",
    "\n",
    "    df_landcover_percent = df_landcover_percent.sort_values(by=['siteId']).reset_index(drop=True)\n",
    "\n",
    "    return df_landcover_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Merge the landcover class into 7 group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_landcover_class(df_landcover_percent):\n",
    "\n",
    "    Landcover_osgb = gpd.read_file(\"Land_cover_shp/land_cover_osgb36.shp\")\n",
    "    landcover_class = sorted(list(set(Landcover_osgb.class_2018)))\n",
    "\n",
    "    urban_residential_area = [landcover_class[2],landcover_class[3], landcover_class[4], landcover_class[5], landcover_class[6], landcover_class[12]]\n",
    "    road_railway = [landcover_class[7], landcover_class[15], landcover_class[18]]\n",
    "    green_urban_area = [landcover_class[9], landcover_class[19]]\n",
    "    natural_area = [landcover_class[0], landcover_class[8], landcover_class[10], landcover_class[16], landcover_class[20]]\n",
    "    other = [landcover_class[1], landcover_class[13],landcover_class[14], landcover_class[17]]\n",
    "\n",
    "    for i in list(df_landcover_percent):\n",
    "        if i in urban_residential_area:\n",
    "            df_landcover_percent = df_landcover_percent.rename(columns={i: 'Urban residential areas'})\n",
    "        elif i in road_railway:\n",
    "            df_landcover_percent = df_landcover_percent.rename(columns={i: 'Roads and railways'})\n",
    "        elif i in green_urban_area:\n",
    "            df_landcover_percent = df_landcover_percent.rename(columns={i: 'Green urban areas (including Sports and leisure facilities)'})\n",
    "        elif i in natural_area:\n",
    "            df_landcover_percent = df_landcover_percent.rename(columns={i: 'Natural areas (Arable land, forests, herbaceous vegetation associations, pastures and water)'})\n",
    "        elif i in other:\n",
    "            df_landcover_percent = df_landcover_percent.rename(columns={i: 'Other (Construction sites, land without current use, mineral extraction and dump sites and port areas)'})\n",
    "\n",
    "    landcover_percent_merge = df_landcover_percent.groupby(df_landcover_percent.columns, axis=1).sum()\n",
    "    landcover_percent_merge = landcover_percent_merge[['siteId', 'Industrial, commercial, public, military and private units',\n",
    "                                                             'Roads and railways', 'Urban residential areas',\n",
    "                                                             'Green urban areas (including Sports and leisure facilities)',\n",
    "                                                             'Natural areas (Arable land, forests, herbaceous vegetation associations, pastures and water)',\n",
    "                                                             'Other (Construction sites, land without current use, mineral extraction and dump sites and port areas)']]\n",
    "    \n",
    "    return landcover_percent_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Get the degree of mixing land use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landuse_mix(landcover_percent_merge):\n",
    "\n",
    "    landcover_percent_merge['landuse_mix'] = 0\n",
    "    \n",
    "    for i in range(len(landcover_percent_merge)):\n",
    "        landuse_percent = landcover_percent_merge.iloc[i,1:].tolist()\n",
    "        landuse_percent = [x for x in landuse_percent if x != 0]\n",
    "        landuse_mix = entropy(landuse_percent)/np.log(len(landuse_percent))\n",
    "        landcover_percent_merge.loc[i, 'landuse_mix'] = landuse_mix\n",
    "    \n",
    "    all_landcover_percent_merge = landcover_percent_merge.fillna(0)\n",
    "    return all_landcover_percent_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Get statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sta(df_landcover_percent, buffer_size):\n",
    "    landcover_sta = df_landcover_percent.describe().T[['mean','std','min','max']]\n",
    "    landcover_sta = landcover_sta.add_suffix('_' + buffer_size)\n",
    "    return landcover_sta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Get landcover percent of all buffer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = ['100', '200', '300', '400']\n",
    "df_list = []\n",
    "for doc in doc_list:\n",
    "    landcover_intersect = read_landcover_intersect_file('useful_detector_buffer_landcover_intersection_' + doc)\n",
    "    df_landcover_percent = get_landcover_percent(landcover_intersect)\n",
    "    landcover_percent_merge = merge_landcover_class(df_landcover_percent)\n",
    "    all_landcover_percent_merge = get_landuse_mix(landcover_percent_merge)\n",
    "    \n",
    "    site_list =  all_landcover_percent_merge.siteId.tolist()\n",
    "    all_landcover_percent_merge = all_landcover_percent_merge.drop('siteId', axis=1).add_suffix('_' + doc)\n",
    "    \n",
    "    df_list.append(all_landcover_percent_merge)\n",
    "\n",
    "all_landcover = pd.concat(df_list, axis=1)\n",
    "all_landcover.insert(loc=0, column='siteId', value=site_list)\n",
    "all_landcover\n",
    "\n",
    "#all_landcover.to_csv('all_buffer_landcover_percent_new.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = ['100', '200', '300', '400']\n",
    "df_list = []\n",
    "for doc in doc_list:\n",
    "    landcover_intersect = read_landcover_intersect_file('useful_detector_buffer_landcover_intersection_' + doc)\n",
    "    df_landcover_percent = get_landcover_percent(landcover_intersect)\n",
    "    landcover_percent_merge = merge_landcover_class(df_landcover_percent)\n",
    "    all_landcover_percent_merge = get_landuse_mix(landcover_percent_merge)\n",
    "    landcover_sta = get_sta(all_landcover_percent_merge, doc)\n",
    "    df_list.append(landcover_sta)\n",
    "\n",
    "all_sta = pd.concat(df_list, axis=1)\n",
    "all_sta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Make defination "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read road intersect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_road_intersect_file(buffer_road_intersect_csv):\n",
    "    road_link_intersect = gpd.read_file(\"../20211028_SCOOT_Data/SCOOT_data_points-shp/Spatial Join/\" + buffer_road_intersect_csv + \".csv\")\n",
    "    return road_link_intersect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Get road density(km/sq.km) of each road group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_road_density(road_link_intersect):     \n",
    "    \n",
    "    sites = list(set(road_link_intersect.siteId))\n",
    "\n",
    "    df_list = []\n",
    "    for site in sites:\n",
    "        each_site_road = road_link_intersect.loc[road_link_intersect['siteId'] == site].loc[:,['routeHiera','LENGTH']]\n",
    "        road_dict = dict(each_site_road.values)\n",
    "        road_dict['siteId'] = site\n",
    "        road_df = pd.DataFrame(road_dict, index=[site])\n",
    "        df_list.append(road_df) \n",
    "\n",
    "    df_road_percent = pd.concat(df_list, ignore_index=True, sort=False).fillna(0)\n",
    "\n",
    "    #change the column order\n",
    "    site_list = df_road_percent.siteId.tolist()\n",
    "    df_road_percent = df_road_percent.drop('siteId', axis=1).astype(float) #convert to float\n",
    "    df_road_percent = df_road_percent/(float(road_link_intersect.BUFF_DIST[0])**2*3.14/1000)\n",
    "    df_road_percent.insert(loc=0, column='siteId', value=site_list)\n",
    "    \n",
    "    \n",
    "    #merge the road into group\n",
    "    df_road_percent['motorway(km/sq.km)']=df_road_percent['Motorway']\n",
    "    df_road_percent['majorRoad(km/sq.km)']=df_road_percent['A Road']+df_road_percent['A Road Primary']\n",
    "    df_road_percent['secondaryRoad(km/sq.km)']=df_road_percent['B Road']+df_road_percent['Minor Road']\n",
    "    df_road_percent['localRoad(km/sq.km)']=df_road_percent['Local Road']+df_road_percent['Local Access Road']+df_road_percent['Restricted Local Access Road']+df_road_percent['Secondary Access Road']+df_road_percent['Restricted Secondary Access Road']\n",
    "    df_road_density = df_road_percent[['siteId','motorway(km/sq.km)','majorRoad(km/sq.km)','secondaryRoad(km/sq.km)','localRoad(km/sq.km)']]\n",
    "\n",
    "    df_road_density = df_road_density.sort_values(by=['siteId']).reset_index(drop=True)\n",
    "    \n",
    "    return df_road_density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Get statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sta(df_road_density,buffer_size):\n",
    "    road_sta = df_road_density.describe().T[['mean','std','min','max']]\n",
    "    road_sta = road_sta.add_suffix('_' + buffer_size)\n",
    "    return road_sta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Get road density of all buffer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = ['100', '200', '300', '400']\n",
    "df_list = []\n",
    "for doc in doc_list:\n",
    "    road_link_intersect = read_road_intersect_file('road_intersection_' + doc)\n",
    "    df_road_density = get_road_density(road_link_intersect)\n",
    "    \n",
    "    site_list =  df_road_density.siteId.tolist()\n",
    "    df_road_density = df_road_density.drop('siteId', axis=1).add_suffix('_' + doc)\n",
    "    \n",
    "    df_list.append(df_road_density)\n",
    "\n",
    "all_road = pd.concat(df_list, axis=1)\n",
    "all_road.insert(loc=0, column='siteId', value=site_list)\n",
    "all_road\n",
    "\n",
    "#all_road.to_csv('all_buffer_road_percent.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = ['100', '200', '300', '400']\n",
    "df_list = []\n",
    "for doc in doc_list:\n",
    "    road_link_intersect = read_road_intersect_file('road_intersection_' + doc)\n",
    "    df_road_density = get_road_density(road_link_intersect)\n",
    "    road_sta = get_sta(df_road_density, doc)\n",
    "    df_list.append(road_sta)\n",
    "\n",
    "all_sta = pd.concat(df_list, axis=1)\n",
    "all_sta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Make defination "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read census csv & Reconstruct the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_census_csv(path):    \n",
    "    #get output area index\n",
    "    index = pd.read_csv('census2011_csv/Index/OA_TO_HIGHER_AREAS.csv')[['OutputArea2011Code', 'CouncilArea2011Code']]\n",
    "    GCC_index = index.loc[index['CouncilArea2011Code'] == 'S12000046'].reset_index(drop=True) # S12000046 for GCC area\n",
    "    GCC_index_list = GCC_index.OutputArea2011Code.tolist()\n",
    "\n",
    "    directory = os.fsencode(path)\n",
    "    all_census = []\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "\n",
    "        if filename.endswith(\".csv\"): \n",
    "            #print(filename)\n",
    "\n",
    "            census_csv = pd.read_csv(path + filename)\n",
    "            census_csv.iloc[3,0] = census_csv.iloc[0,0]\n",
    "            census_csv.columns = census_csv.iloc[3]\n",
    "            census_csv = census_csv.add_prefix(census_csv.columns[0].split('-')[1]+': ')\n",
    "            census_csv = census_csv.rename(columns={census_csv.columns[0]: 'OutputArea2011Code'})\n",
    "\n",
    "            df_list = []\n",
    "            for each_index in GCC_index_list:\n",
    "                GCC_census_each = census_csv.loc[census_csv['OutputArea2011Code'] == each_index]\n",
    "                df_list.append(GCC_census_each)\n",
    "\n",
    "            GCC_census = pd.concat(df_list).reset_index(drop=True)\n",
    "            all_census.append(GCC_census)\n",
    "\n",
    "    dfs = [df.set_index('OutputArea2011Code') for df in all_census]\n",
    "    all_census_csv = pd.concat(dfs, axis=1)\n",
    "    all_census_csv = all_census_csv.replace('-', 0).replace({',': ''}, regex=True)._convert(numeric=True)\n",
    "    all_census_csv = all_census_csv.rename_axis(\"code\").reset_index()\n",
    "    \n",
    "    return all_census_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Get the useful census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_useful_census(all_census_csv):    \n",
    "    \n",
    "    useful_census_csv = all_census_csv[['code',' Age structure: Mean age',' Age structure: Median age',\n",
    "                    ' Usual resident population: All people',' Usual resident population: Males',' Ethnic group: White',\n",
    "                    ' Qualifications and students: All people aged 16 and over: Highest level of qualification: Level 4 qualifications and above',\n",
    "                    ' Distance travelled (1) to work: All people aged 16 to 74 in employment',\n",
    "                    ' Method of travel to work or study (1): Driving a car or van',' Method of travel to work or study (1): Passenger in a car or van',\n",
    "                    ' Method of travel to work or study (1): Bicycle',' Method of travel to work or study (1): On foot']]\n",
    "    useful_census_csv[' Method of travel to work or study (1): Car or van'] = useful_census_csv[' Method of travel to work or study (1): Driving a car or van']+useful_census_csv[' Method of travel to work or study (1): Passenger in a car or van']\n",
    "    useful_census_csv = useful_census_csv.drop([' Method of travel to work or study (1): Driving a car or van',' Method of travel to work or study (1): Passenger in a car or van'], axis=1)\n",
    "    \n",
    "    OA_shp = gpd.read_file(\"census2011_shp/output-area-2011-eor/OutputArea2011_EoR_GCC.shp\")[['code','SHAPE_1_Ar']]\n",
    "    useful_census_csv_area = pd.merge(OA_shp, useful_census_csv,how=\"right\", on='code')\n",
    "    useful_census_csv_area.iloc[:,4:] = useful_census_csv_area.iloc[:,4:].div(useful_census_csv_area.SHAPE_1_Ar, axis=0)\n",
    "    \n",
    "    return useful_census_csv_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Read output area intersect (detetor buffer) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_OA_intersect_file(buffer_OA_intersect_csv):\n",
    "    OA_link_intersect = gpd.read_file(\"../20211028_SCOOT_Data/SCOOT_data_points-shp/Spatial Join/\" + buffer_OA_intersect_csv + \".csv\")\n",
    "    return OA_link_intersect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Merge useful census data to intersect csv and calculate each percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_census_percent(OA_link_intersect, useful_census_csv_area):    \n",
    "    census_intersect = pd.merge(OA_link_intersect[['siteId','code','AREA','PERCENTAGE']], useful_census_csv_area,how=\"left\", on='code')\n",
    "    census_intersect.iloc[:,5:7] = census_intersect.iloc[:,5:7].multiply(census_intersect[\"PERCENTAGE\"]._convert(numeric=True),axis=\"index\")/100\n",
    "    census_intersect.iloc[:,7:] = census_intersect.iloc[:,7:].multiply(census_intersect[\"AREA\"]._convert(numeric=True),axis=\"index\")\n",
    "\n",
    "    sites = list(set(OA_link_intersect.siteId))\n",
    "    df_list = []\n",
    "    for site in sites:\n",
    "        #print(site)\n",
    "        each_census_sum = census_intersect.loc[census_intersect.siteId == site].iloc[:,5:].sum()\n",
    "        each_census_df = each_census_sum.to_frame().T  #convert series to df\n",
    "        each_census_df.insert(loc=0, column='siteId', value=[site])\n",
    "\n",
    "        #calculate the percentage of each column within each census group\n",
    "        each_census_df['mean_age'] = each_census_df[' Age structure: Mean age']\n",
    "        each_census_df['median_age'] = each_census_df[' Age structure: Median age']\n",
    "        each_census_df['population_density(persons/sq.km)'] = each_census_df[' Usual resident population: All people']/(float(OA_link_intersect.BUFF_DIST[0])**2*3.14/1000000)\n",
    "        each_census_df['employment_density(persons/sq.km)'] = each_census_df[' Distance travelled (1) to work: All people aged 16 to 74 in employment']/(float(OA_link_intersect.BUFF_DIST[0])**2*3.14/1000000)\n",
    "        each_census_df['male_percent'] = each_census_df[' Usual resident population: Males']/each_census_df[' Usual resident population: All people']\n",
    "        each_census_df['white_percent'] = each_census_df[' Ethnic group: White']/each_census_df[' Usual resident population: All people']\n",
    "        each_census_df['collegeDegree_percent'] = each_census_df[' Qualifications and students: All people aged 16 and over: Highest level of qualification: Level 4 qualifications and above']/each_census_df[' Usual resident population: All people']\n",
    "        each_census_df['bicycle_percent(commute)'] = each_census_df[' Method of travel to work or study (1): Bicycle']/each_census_df[' Usual resident population: All people']\n",
    "        each_census_df['walk_percent(commute)'] = each_census_df[' Method of travel to work or study (1): On foot']/each_census_df[' Usual resident population: All people']\n",
    "        each_census_df['car_percent(commute)'] = each_census_df[' Method of travel to work or study (1): Car or van']/each_census_df[' Usual resident population: All people']\n",
    "        each_census_percent = each_census_df[['siteId', 'mean_age', 'median_age', 'population_density(persons/sq.km)', 'employment_density(persons/sq.km)', \n",
    "                 'male_percent', 'white_percent', 'collegeDegree_percent', 'bicycle_percent(commute)', \n",
    "                 'walk_percent(commute)', 'car_percent(commute)']]\n",
    "\n",
    "        df_list.append(each_census_percent)\n",
    "\n",
    "    df_census_percent = pd.concat(df_list)\n",
    "    df_census_percent = df_census_percent.sort_values(by=['siteId']).reset_index(drop=True)\n",
    "    \n",
    "    return df_census_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Get statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sta(df_census_percent, buffer_size):\n",
    "    census_sta = df_census_percent.describe().T[['mean','std','min','max']]\n",
    "    census_sta = census_sta.add_suffix('_' + buffer_size)\n",
    "    return census_sta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Get census of all buffer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = ['100', '200', '300', '400']\n",
    "df_list = []\n",
    "\n",
    "all_census_csv = reconstruct_census_csv('census2011_csv/useful_data/')\n",
    "useful_census_csv_area = get_useful_census(all_census_csv)\n",
    "\n",
    "for doc in doc_list:\n",
    "    OA_link_intersect = read_OA_intersect_file('census_OA_intersection_' + doc)\n",
    "    df_census_percent = get_census_percent(OA_link_intersect, useful_census_csv_area)\n",
    "    \n",
    "    site_list =  df_census_percent.siteId.tolist()\n",
    "    df_census_percent = df_census_percent.drop('siteId', axis=1).add_suffix('_' + doc)\n",
    "    \n",
    "    df_list.append(df_census_percent)\n",
    "\n",
    "all_census = pd.concat(df_list, axis=1)\n",
    "all_census.insert(loc=0, column='siteId', value=site_list)\n",
    "all_census\n",
    "\n",
    "#all_census.to_csv('all_buffer_census_percent.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = ['100', '200', '300', '400']\n",
    "df_list = []\n",
    "for doc in doc_list:\n",
    "    OA_link_intersect = read_OA_intersect_file('census_OA_intersection_' + doc)\n",
    "    df_census_percent = get_census_percent(OA_link_intersect, useful_census_csv_area)\n",
    "    census_sta = get_sta(df_census_percent, doc)\n",
    "    df_list.append(census_sta)\n",
    "\n",
    "all_sta = pd.concat(df_list, axis=1)\n",
    "all_sta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
