{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cRiapZ1P3wy"
   },
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "code",
    "id": "kAbdmRmvq0Je"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\program\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from io import BytesIO\n",
    "import tarfile\n",
    "import tempfile\n",
    "from six.moves import urllib\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#%tensorflow_version 1.x\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p47cYGGOQE1W"
   },
   "source": [
    "## 2. Import helper methods\n",
    "These methods help us perform the following tasks:\n",
    "* Load the latest version of the pretrained DeepLab model\n",
    "* Load the colormap from the PASCAL VOC dataset\n",
    "* Adds colors to various labels, such as \"pink\" for people, \"green\" for bicycle and more\n",
    "* Visualize an image, and add an overlay of colors on various regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "code",
    "id": "vN0kU6NJ1Ye5"
   },
   "outputs": [],
   "source": [
    "class DeepLabModel(object):\n",
    "    \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
    "\n",
    "    INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
    "    OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
    "    INPUT_SIZE = 513\n",
    "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
    "\n",
    "    def __init__(self, tarball_path):\n",
    "        \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
    "        self.graph = tf.Graph()\n",
    "\n",
    "        graph_def = None\n",
    "        # Extract frozen graph from tar archive.\n",
    "        tar_file = tarfile.open(tarball_path)\n",
    "        for tar_info in tar_file.getmembers():\n",
    "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
    "                file_handle = tar_file.extractfile(tar_info)\n",
    "                graph_def = tf.GraphDef.FromString(file_handle.read())\n",
    "                break\n",
    "\n",
    "        tar_file.close()\n",
    "\n",
    "        if graph_def is None:\n",
    "            raise RuntimeError('Cannot find inference graph in tar archive.')\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "\n",
    "    def run(self, image):\n",
    "        \"\"\"Runs inference on a single image.\n",
    "\n",
    "        Args:\n",
    "          image: A PIL.Image object, raw input image.\n",
    "\n",
    "        Returns:\n",
    "          resized_image: RGB image resized from original input image.\n",
    "          seg_map: Segmentation map of `resized_image`.\n",
    "        \"\"\"\n",
    "        width, height = image.size\n",
    "        resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n",
    "        target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
    "        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
    "        batch_seg_map = self.sess.run(\n",
    "            self.OUTPUT_TENSOR_NAME,\n",
    "            feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
    "        seg_map = batch_seg_map[0]\n",
    "        return resized_image, seg_map\n",
    "\n",
    "\n",
    "def create_cityscapes_label_colormap():\n",
    "    \"\"\"Creates a label colormap used in CITYSCAPES segmentation benchmark.\n",
    "\n",
    "  Returns:\n",
    "    A Colormap for visualizing segmentation results.\n",
    "  \"\"\"\n",
    "    colormap = np.zeros((256, 3), dtype=np.uint8)\n",
    "    colormap[0] = [128, 64, 128]\n",
    "    colormap[1] = [244, 35, 232]\n",
    "    colormap[2] = [70, 70, 70]\n",
    "    colormap[3] = [102, 102, 156]\n",
    "    colormap[4] = [190, 153, 153]\n",
    "    colormap[5] = [153, 153, 153]\n",
    "    colormap[6] = [250, 170, 30]\n",
    "    colormap[7] = [220, 220, 0]\n",
    "    colormap[8] = [107, 142, 35]\n",
    "    colormap[9] = [152, 251, 152]\n",
    "    colormap[10] = [70, 130, 180]\n",
    "    colormap[11] = [220, 20, 60]\n",
    "    colormap[12] = [255, 0, 0]\n",
    "    colormap[13] = [0, 0, 142]\n",
    "    colormap[14] = [0, 0, 70]\n",
    "    colormap[15] = [0, 60, 100]\n",
    "    colormap[16] = [0, 80, 100]\n",
    "    colormap[17] = [0, 0, 230]\n",
    "    colormap[18] = [119, 11, 32]\n",
    "\n",
    "    return colormap\n",
    "\n",
    "\n",
    "def label_to_color_image(label):\n",
    "    \"\"\"Adds color defined by the dataset colormap to the label.\n",
    "\n",
    "    Args:\n",
    "    label: A 2D array with integer type, storing the segmentation label.\n",
    "\n",
    "    Returns:\n",
    "    result: A 2D array with floating type. The element of the array\n",
    "      is the color indexed by the corresponding element in the input label\n",
    "      to the PASCAL color map.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If label is not of rank 2 or its value is larger than color\n",
    "      map maximum entry.\n",
    "    \"\"\"\n",
    "    if label.ndim != 2:\n",
    "        raise ValueError('Expect 2-D input label')\n",
    "\n",
    "    colormap = create_cityscapes_label_colormap()\n",
    "\n",
    "    if np.max(label) >= len(colormap):\n",
    "        raise ValueError('label value too large.')\n",
    "\n",
    "    return colormap[label]\n",
    "\n",
    "\n",
    "def vis_segmentation(image, seg_map):\n",
    "    \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n",
    "\n",
    "    plt.subplot(grid_spec[0])\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title('input image')\n",
    "\n",
    "    plt.subplot(grid_spec[1])\n",
    "    seg_image = label_to_color_image(seg_map).astype(np.uint8)\n",
    "    plt.imshow(seg_image)\n",
    "    plt.axis('off')\n",
    "    plt.title('segmentation map')\n",
    "\n",
    "    plt.subplot(grid_spec[2])\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(seg_image, alpha=0.7)\n",
    "    plt.axis('off')\n",
    "    plt.title('segmentation overlay')\n",
    "\n",
    "    unique_labels = np.unique(seg_map)\n",
    "    ax = plt.subplot(grid_spec[3])\n",
    "    plt.imshow(\n",
    "      FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n",
    "    ax.yaxis.tick_right()\n",
    "    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n",
    "    plt.xticks([], [])\n",
    "    ax.tick_params(width=0.0)\n",
    "    plt.grid('off')\n",
    "    \n",
    "    save_path = img_loc.split('/')[-1]\n",
    "    plt.savefig('semantic segmentation model/results/' + save_path)\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "LABEL_NAMES = np.asarray([\n",
    "    'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
    "    'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck',\n",
    "    'bus', 'train', 'motorcycle', 'bicycle'\n",
    "])\n",
    "\n",
    "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
    "FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGcZzNkASG9A"
   },
   "source": [
    "## 3. Select a pretrained model\n",
    "We have trained the DeepLab model using various backbone networks. Select one from the MODEL_NAME list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4oXKmnjw6i_",
    "outputId": "a837a5ab-162e-4993-d1ff-d972c5fb9c85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DeepLabModel at 0x1dbe0f55fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'semantic segmentation model/deeplab_cityscapes_xception71_trainfine_2018_09_08.tar.gz'\n",
    "MODEL = DeepLabModel(model_path)\n",
    "MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZst78N-4OKO"
   },
   "source": [
    "## 4. Run on sample images\n",
    "\n",
    "Select one of sample images (leave `IMAGE_URL` empty) or feed any internet image\n",
    "url for inference.\n",
    "\n",
    "Note that this colab uses single scale inference for fast computation,\n",
    "so the results may slightly differ from the visualizations in the\n",
    "[README](https://github.com/tensorflow/models/blob/master/research/deeplab/README.md) file,\n",
    "which uses multi-scale and left-right flipped inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "edGukUHXyymr",
    "outputId": "23587ad8-a275-4c3b-ab0e-180eb961ddd3"
   },
   "outputs": [],
   "source": [
    "#image_loc = 'data/Latest_SCOOT_GSV/55.848209730275_-4.255344777421482_180.jpg'\n",
    "\n",
    "\n",
    "def run_visualization(url):\n",
    "    \"\"\"Inferences DeepLab model and visualizes result.\"\"\"\n",
    "    try:\n",
    "        original_im = Image.open(url)\n",
    "    except IOError:\n",
    "        print('Cannot retrieve image. Please check loc: ' + url)\n",
    "        return\n",
    "\n",
    "    print('running deeplab on image %s...' % url)\n",
    "    resized_im, seg_map = MODEL.run(original_im)\n",
    "\n",
    "    \"\"\"Get percentage of each object in seg_map\"\"\"\n",
    "    labels, counts = np.unique(seg_map, return_counts=True)\n",
    "    percentage = counts / counts.sum()\n",
    "    objects = LABEL_NAMES[labels]\n",
    "    object_percentage = pd.DataFrame({'object': objects, 'percentage': percentage}, columns=['object', 'percentage'])\n",
    "    object_percentage_t = object_percentage.T\n",
    "    \n",
    "    object_percentage_t.columns = object_percentage_t.loc['object']\n",
    "    object_percentage_t = object_percentage_t.drop(['object'])\n",
    "    object_percentage_t\n",
    "    #display(object_percentage_t)\n",
    "\n",
    "    images = vis_segmentation(resized_im, seg_map)\n",
    "\n",
    "    return object_percentage_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the GSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GSV download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download gsv\n",
    "def get_gsv(lat, lon, heading, key, width=600, height=400):\n",
    "    # heading: north: 0, east: 90, south: 180, west: 270\n",
    "    url = \"https://maps.googleapis.com/maps/api/streetview?size={}x{}&location={},{}&heading={}&key={}\".format(width, height, lat, lon, heading, key)\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# if you download the raw traffic flow data via Traffic_flow_Download.ipynb and pre-process the data via Traffic_flow_Preprocessing.ipynb\n",
    "SCOOT_df = pd.read_csv('../data/final_regression_csv/detector_530.csv')\n",
    "SCOOT_df_coords = SCOOT_df.loc[:,['siteId', 'lat', 'lon']]\n",
    "\n",
    "# elif you download the pre-processed traffic flow data at: Urban Big Data Centre (UBDC) and https://zenodo.org/records/12100278.\n",
    "SCOOT_df = pd.read_csv('../data/traffic_flow_data/locations.csv')\n",
    "SCOOT_df_coords = SCOOT_df.loc[:,['id', 'latitude', 'longitude']]\n",
    "SCOOT_df_coords = SCOOT_df_coords.rename(columns={'id': 'siteId', 'latitude': 'lat', 'longitude': 'lon'})\n",
    "\n",
    "GSV_list = []\n",
    "for i in range(len(SCOOT_df_coords)):\n",
    "    lat = SCOOT_df_coords.loc[i]['lat']\n",
    "    lon = SCOOT_df_coords.loc[i]['lon']\n",
    "    for j in ['0', '90', '180', '270']:\n",
    "        view = get_gsv(lat, lon, j, key) # get your unique GSV download code https://developers.google.com/maps/documentation/streetview/overview\n",
    "        view.save('data/' + lat + '_' + lon + '_' + j + '.jpg')\n",
    "        \n",
    "        GSV_download = SCOOT_df_coords.loc[[i]]\n",
    "        GSV_download['path'] = 'data/' + str(lat) + '_' + str(lon) + '_' + str(j) + '.jpg'\n",
    "        GSV_list.append(GSV_download)\n",
    "\n",
    "GSV_download_df = pd.concat(GSV_list)\n",
    "GSV_download_df = GSV_download_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSV_download_df[['road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
    "                 'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck',\n",
    "                 'bus', 'train', 'motorcycle', 'bicycle']] = pd.DataFrame([[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]], \n",
    "                                                                          index=GSV_download_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement the semantic segmentation on GSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(file_for_segmentation)):\n",
    "    sum_row = file_for_segmentation.iloc[i:i+1, 4:].sum(axis=1).item()\n",
    "    if sum_row == 0:\n",
    "        img_loc = file_for_segmentation.path[i]\n",
    "        segmentation_output = run_visualization(img_loc)\n",
    "        for j in list(segmentation_output):\n",
    "            file_for_segmentation.at[i, j] = segmentation_output[j].item()\n",
    "        \n",
    "        file_for_segmentation.to_csv('semantic segmentation model/latest_segmentation_output_before_2021-12-31.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge the four perspective into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisation_GSV(data):\n",
    "\n",
    "    site_list = list(set(data.siteId))\n",
    "    objects = list(data)[4:]\n",
    "    #print(len(date_list))\n",
    "\n",
    "    site_object = {}\n",
    "    for i in site_list:\n",
    "        object_value = []\n",
    "        for j in objects:\n",
    "            b = data.loc[data.siteId == i, j].sum()/4\n",
    "            object_value.append(b)\n",
    "        site_object[i] = object_value\n",
    "\n",
    "    each_site_object = {}\n",
    "    site = []\n",
    "    \n",
    "    road = []\n",
    "    sidewalk = []\n",
    "    building = []\n",
    "    wall = []\n",
    "    fence = []\n",
    "    \n",
    "    pole = []\n",
    "    traffic_light = []\n",
    "    traffic_sign = []\n",
    "    vegetation = []\n",
    "    terrain = []\n",
    "    \n",
    "    sky = []\n",
    "    person = []\n",
    "    rider = []\n",
    "    car = []\n",
    "    truck = []\n",
    "    \n",
    "    bus = []\n",
    "    train = []\n",
    "    motorcycle = []\n",
    "    bicycle = []\n",
    "    \n",
    "    for key, value in site_object.items():\n",
    "        site.append(key)\n",
    "        \n",
    "        road.append(value[0])\n",
    "        sidewalk.append(value[1])\n",
    "        building.append(value[2])\n",
    "        wall.append(value[3])\n",
    "        fence.append(value[4])\n",
    "\n",
    "        pole.append(value[5])\n",
    "        traffic_light.append(value[6])\n",
    "        traffic_sign.append(value[7])\n",
    "        vegetation.append(value[8])\n",
    "        terrain.append(value[9])\n",
    "\n",
    "        sky.append(value[10])\n",
    "        person.append(value[11])\n",
    "        rider.append(value[12])\n",
    "        car.append(value[13])\n",
    "        truck.append(value[14])\n",
    "\n",
    "        bus.append(value[15])\n",
    "        train.append(value[16])\n",
    "        motorcycle.append(value[17])\n",
    "        bicycle.append(value[18])\n",
    "        \n",
    "\n",
    "    each_site_object['siteId'] = site\n",
    "    each_site_object['road'] = road\n",
    "    each_site_object['sidewalk'] = sidewalk\n",
    "    each_site_object['building'] = building\n",
    "    each_site_object['wall'] = wall\n",
    "    each_site_object['fence'] = fence\n",
    "    \n",
    "    each_site_object['pole'] = pole\n",
    "    each_site_object['traffic light'] = traffic_light\n",
    "    each_site_object['traffic sign'] = traffic_sign\n",
    "    each_site_object['vegetation'] = vegetation\n",
    "    each_site_object['terrain'] = terrain\n",
    "    \n",
    "    each_site_object['sky'] = sky\n",
    "    each_site_object['person'] = person\n",
    "    each_site_object['rider'] = rider\n",
    "    each_site_object['car'] = car\n",
    "    each_site_object['truck'] = truck\n",
    "    \n",
    "    each_site_object['bus'] = bus\n",
    "    each_site_object['train'] = train\n",
    "    each_site_object['motorcycle'] = motorcycle\n",
    "    each_site_object['bicycle'] = bicycle\n",
    "    \n",
    "    df_each_site_object = pd.DataFrame.from_dict(each_site_object)\n",
    "    \n",
    "    df_each_site_object = df_each_site_object.sort_values(by=['siteId'])\n",
    "    df_GSV = df_each_site_object.reset_index(drop=True)\n",
    "    \n",
    "    return df_GSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "new_GSV = normalisation_GSV(GSV)\n",
    "save_path = '../data/independent_variables/GSV/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "new_GSV.to_csv(save_path + 'segmentation_normalisation_output.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
